{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import cv2\n",
    "import diffusers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from diffusers import LMSDiscreteScheduler, StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "from skimage.exposure import match_histograms\n",
    "from torch import autocast\n",
    "from torchvision import transforms as tfms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from img2img import StableDiffusionImg2ImgPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STABLE_DIFFUSION_MODEL_PATH = Path.home() / \"Desktop/stable-diffusion-v1-4\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'trained_betas'} was not found in config. Values will be initialized to default values.\n",
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusionImg2ImgPipeline {\n",
       "  \"_class_name\": \"StableDiffusionImg2ImgPipeline\",\n",
       "  \"_diffusers_version\": \"0.3.0\",\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"LMSDiscreteScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the txt_to_img pipeline\n",
    "txt_to_img = StableDiffusionPipeline.from_pretrained(\n",
    "    str(STABLE_DIFFUSION_MODEL_PATH), revision=\"fp16\", torch_dtype=torch.float16\n",
    ")\n",
    "# Turn off safety_checker to avoid false positives\n",
    "txt_to_img.safety_checker = lambda images, **kwargs: (images, False)\n",
    "# txt_to_img.enable_attention_slicing()  # use less vram\n",
    "txt_to_img = txt_to_img.to(device)\n",
    "# Load the img2img pipeline, using the models\n",
    "# from the txt_to_img pipeline, to not waste vram.\n",
    "im2im = StableDiffusionImg2ImgPipeline(\n",
    "    vae=txt_to_img.vae,\n",
    "    text_encoder=txt_to_img.text_encoder,\n",
    "    tokenizer=txt_to_img.tokenizer,\n",
    "    unet=txt_to_img.unet,\n",
    "    scheduler=LMSDiscreteScheduler(\n",
    "        beta_start=0.00085,\n",
    "        beta_end=0.012,\n",
    "        beta_schedule=\"scaled_linear\",\n",
    "        num_train_timesteps=1000,\n",
    "    ),\n",
    ")\n",
    "# im2im.enable_attention_slicing()\n",
    "im2im.progress_bar = lambda iterable: iterable  # turn off progress bar\n",
    "im2im.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "\n",
    "\n",
    "def timestamp():\n",
    "    return time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "\n",
    "def maintain_colors(prev_img, color_match_sample, mode):\n",
    "    # source: https://colab.research.google.com/github/deforum/stable-diffusion/blob/main/Deforum_Stable_Diffusion.ipynb#scrollTo=2g-f7cQmf2Nt\n",
    "    if mode == \"Match Frame 0 RGB\":\n",
    "        return match_histograms(prev_img, color_match_sample, multichannel=True)\n",
    "    elif mode == \"Match Frame 0 HSV\":\n",
    "        prev_img_hsv = cv2.cvtColor(prev_img, cv2.COLOR_RGB2HSV)\n",
    "        color_match_hsv = cv2.cvtColor(color_match_sample, cv2.COLOR_RGB2HSV)\n",
    "        matched_hsv = match_histograms(prev_img_hsv, color_match_hsv, multichannel=True)\n",
    "        return cv2.cvtColor(matched_hsv, cv2.COLOR_HSV2RGB)\n",
    "    else:  # Match Frame 0 LAB\n",
    "        prev_img_lab = cv2.cvtColor(prev_img, cv2.COLOR_RGB2LAB)\n",
    "        color_match_lab = cv2.cvtColor(color_match_sample, cv2.COLOR_RGB2LAB)\n",
    "        matched_lab = match_histograms(prev_img_lab, color_match_lab, multichannel=True)\n",
    "        return cv2.cvtColor(matched_lab, cv2.COLOR_LAB2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notebook_path.resolve() = PosixPath('/home/sid/Documents/stable-diffusion-im2im/main.ipynb')\n",
      "OUTPUT_DIR.resolve() = PosixPath('/home/sid/Documents/stable-diffusion-im2im/images/20221002-151033')\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = Path(f\"images/{timestamp()}\")\n",
    "PROMPT_A = \"A photo of a bowl of fruit\"\n",
    "PROMPT_B = \"A photo of an acrobat\"\n",
    "GUIDANCE_SCALE = 5\n",
    "IMG2IMG_STRENGTH = 0.7\n",
    "NUM_IMG2IMG_STEPS = 20\n",
    "SEED = 0\n",
    "WIDTH = 512\n",
    "HEIGHT = 512\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "# Use same random seed for everything\n",
    "generator = torch.Generator(\"cuda\").manual_seed(SEED)\n",
    "# To know where things were\n",
    "notebook_path = Path(__vsc_ipynb_file__)  # vscode only\n",
    "print(f\"{notebook_path.resolve() = }\")\n",
    "print(f\"{OUTPUT_DIR.resolve() = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('images/20221002-151033/main_20221002-151034.ipynb')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save a copy of this nb in OUTPUT_DIR, for reproducibility\n",
    "shutil.copy(\n",
    "    src=notebook_path, dst=OUTPUT_DIR / f\"{notebook_path.stem}_{timestamp()}.ipynb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:09<00:00,  5.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate the initial image\n",
    "with autocast(\"cuda\"), torch.no_grad():\n",
    "    init_image = txt_to_img(\n",
    "        [PROMPT_A], width=WIDTH, height=HEIGHT, generator=generator\n",
    "    )[\"sample\"][0]\n",
    "init_image.save(OUTPUT_DIR / f\"{PROMPT_A}_{PROMPT_B}_{0:04d}.jpg\")\n",
    "image = init_image\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:17<00:00,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "i_start = i + 1  # means can rerun cell to continue generation\n",
    "# Generate the rest of the images\n",
    "for i in tqdm(range(i_start, i_start + NUM_IMG2IMG_STEPS)):\n",
    "    # Try to prevent colours from going red\n",
    "    image = Image.fromarray(\n",
    "        maintain_colors(np.array(image), np.array(init_image), \"Match Frame 0 RGB\")\n",
    "    )\n",
    "    generator = torch.Generator(\"cuda\").manual_seed(i)\n",
    "    with autocast(\"cuda\"), torch.no_grad():\n",
    "        image = im2im(\n",
    "            PROMPT_B,\n",
    "            image,\n",
    "            strength=IMG2IMG_STRENGTH,\n",
    "            guidance_scale=GUIDANCE_SCALE,\n",
    "            generator=generator,\n",
    "        )[\"sample\"][0]\n",
    "    image.save(OUTPUT_DIR / f\"{PROMPT_A}_{PROMPT_B}_{i:04d}.jpg\")\n",
    "print(f\"{i = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 10.4.0 (conda-forge gcc 10.4.0-16)\n",
      "  configuration: --prefix=/home/conda/feedstock_root/build_artifacts/ffmpeg_1664281150702/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1664281150702/_build_env/bin/x86_64-conda-linux-gnu-cc --cxx=/home/conda/feedstock_root/build_artifacts/ffmpeg_1664281150702/_build_env/bin/x86_64-conda-linux-gnu-c++ --nm=/home/conda/feedstock_root/build_artifacts/ffmpeg_1664281150702/_build_env/bin/x86_64-conda-linux-gnu-nm --ar=/home/conda/feedstock_root/build_artifacts/ffmpeg_1664281150702/_build_env/bin/x86_64-conda-linux-gnu-ar --disable-doc --disable-openssl --enable-avresample --enable-demuxer=dash --enable-hardcoded-tables --enable-libfreetype --enable-libfontconfig --enable-libopenh264 --enable-gnutls --enable-libmp3lame --enable-libvpx --enable-pthreads --enable-vaapi --enable-gpl --enable-libx264 --enable-libx265 --enable-libaom --enable-libsvtav1 --enable-libxml2 --enable-pic --enable-shared --disable-static --enable-version3 --enable-zlib --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1664281150702/_build_env/bin/pkg-config\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, image2, from 'images/20221002-151033/*.jpg':\n",
      "  Duration: 00:00:02.10, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0: Video: mjpeg (Baseline), yuvj420p(pc, bt470bg/unknown/unknown), 512x512 [SAR 1:1 DAR 1:1], 10 fps, 10 tbr, 10 tbn, 10 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "\u001b[1;34m[swscaler @ 0x561429f0f300] \u001b[0m\u001b[0;33mdeprecated pixel format used, make sure you did set range correctly\n",
      "\u001b[0m\u001b[1;36m[libx264 @ 0x561429ec0a80] \u001b[0musing SAR=1/1\n",
      "\u001b[1;36m[libx264 @ 0x561429ec0a80] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "\u001b[1;36m[libx264 @ 0x561429ec0a80] \u001b[0mprofile High, level 2.2, 4:2:0, 8-bit\n",
      "\u001b[1;36m[libx264 @ 0x561429ec0a80] \u001b[0m264 - core 164 r3095 baee400 - H.264/MPEG-4 AVC codec - Copyleft 2003-2022 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=16 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=10 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'images/20221002-151033/movie.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.76.100\n",
      "  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv420p(tv, bt470bg/unknown/unknown, progressive), 512x512 [SAR 1:1 DAR 1:1], q=2-31, 10 fps, 10240 tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.134.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "frame=   21 fps=0.0 q=-1.0 Lsize=     296kB time=00:00:01.80 bitrate=1346.8kbits/s speed=10.4x    \n",
      "video:295kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.368292%\n",
      "\u001b[1;36m[libx264 @ 0x561429ec0a80] \u001b[0mframe I:2     Avg QP:23.19  size: 25618\n",
      "\u001b[1;36m[libx264 @ 0x561429ec0a80] \u001b[0mframe P:9     Avg QP:22.09  size: 13153\n",
      "\u001b[1;36m[libx264 @ 0x561429ec0a80] \u001b[0mframe B:10    Avg QP:22.02  size: 13163\n",
      "\u001b[1;36m[libx264 @ 0x561429ec0a80] \u001b[0mconsecutive B-frames: 19.0% 47.6% 14.3% 19.0%\n",
      "\u001b[1;36m[libx264 @ 0x561429ec0a80] \u001b[0mmb I  I16..4:  2.0% 93.0%  5.1%\n",
      "\u001b[1;36m[libx264 @ 0x561429ec0a80] \u001b[0mmb P  I16..4:  8.0% 84.7%  3.8%  P16..4:  1.4%  1.3%  0.6%  0.0%  0.0%    skip: 0.0%\n",
      "\u001b[1;36m[libx264 @ 0x561429ec0a80] \u001b[0mmb B  I16..4:  7.0% 63.0%  3.0%  B16..8: 11.9%  5.5%  1.4%  direct: 7.5%  skip: 0.7%  L0:41.3% L1:44.5% BI:14.2%\n",
      "\u001b[1;36m[libx264 @ 0x561429ec0a80] \u001b[0m8x8 transform intra:87.7% inter:87.6%\n",
      "\u001b[1;36m[libx264 @ 0x561429ec0a80] \u001b[0mcoded y,uvDC,uvAC intra: 79.6% 73.8% 23.4% inter: 66.4% 94.8% 26.7%\n",
      "\u001b[1;36m[libx264 @ 0x561429ec0a80] \u001b[0mi16 v,h,dc,p:  5%  5%  3% 86%\n",
      "\u001b[1;36m[libx264 @ 0x561429ec0a80] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 16% 11% 50%  4%  3%  4%  2%  5%  4%\n",
      "\u001b[1;36m[libx264 @ 0x561429ec0a80] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 28% 11% 18%  6% 10% 12%  5%  7%  3%\n",
      "\u001b[1;36m[libx264 @ 0x561429ec0a80] \u001b[0mi8c dc,h,v,p: 39% 24% 28%  9%\n",
      "\u001b[1;36m[libx264 @ 0x561429ec0a80] \u001b[0mWeighted P-Frames: Y:22.2% UV:22.2%\n",
      "\u001b[1;36m[libx264 @ 0x561429ec0a80] \u001b[0mref P L0: 58.1% 23.7% 10.3%  6.7%  1.2%\n",
      "\u001b[1;36m[libx264 @ 0x561429ec0a80] \u001b[0mref B L0: 75.8% 20.8%  3.4%\n",
      "\u001b[1;36m[libx264 @ 0x561429ec0a80] \u001b[0mref B L1: 96.4%  3.6%\n",
      "\u001b[1;36m[libx264 @ 0x561429ec0a80] \u001b[0mkb/s:1147.60\n",
      "VLC media player 3.0.16 Vetinari (revision 3.0.16-0-g5e70837d8d)\n",
      "[\u001b[32;1m0000555fade68a00\u001b[0m] main libvlc: \u001b[0;1mRunning vlc with the default interface. Use 'cvlc' to use vlc without interface.\u001b[0m\n",
      "TagLib: MP4: No audio tracks\n",
      "Qt: Session management error: Could not open network socket\n",
      "TagLib: MP4: No audio tracks\n",
      "libva info: VA-API version 1.1.0\n",
      "libva info: va_getDriverName() returns 0\n",
      "libva info: Trying to open /snap/vlc/2344/usr/lib/x86_64-linux-gnu/dri/nvidia_drv_video.so\n",
      "libva info: Found init function __vaDriverInit_1_0\n",
      "libva info: va_openDriver() returns 0\n",
      "[\u001b[32;1m00007f4800001f30\u001b[0m] glconv_vaapi_x11 gl error: \u001b[31;1mvaCreateSurfaces: attribute not supported\u001b[0m\n",
      "[\u001b[32;1m00007f480804fc10\u001b[0m] main video output error: \u001b[31;1mvideo output creation failed\u001b[0m\n",
      "[\u001b[32;1m00007f4814c1a130\u001b[0m] main decoder error: \u001b[31;1mfailed to create video output\u001b[0m\n",
      "[\u001b[32;1m00007f4814c1a130\u001b[0m] avcodec decoder: \u001b[0;1mUsing NVIDIA VDPAU Driver Shared Library  470.141.03  Thu Jun 30 18:33:34 UTC 2022 for hardware decoding\u001b[0m\n",
      "TagLib: MP4: No audio tracks\n",
      "[\u001b[32;1m00007f4814c062e0\u001b[0m] avcodec decoder: \u001b[0;1mUsing NVIDIA VDPAU Driver Shared Library  470.141.03  Thu Jun 30 18:33:34 UTC 2022 for hardware decoding\u001b[0m\n",
      "TagLib: MP4: No audio tracks\n",
      "[\u001b[32;1m00007f4814c22c60\u001b[0m] avcodec decoder: \u001b[0;1mUsing NVIDIA VDPAU Driver Shared Library  470.141.03  Thu Jun 30 18:33:34 UTC 2022 for hardware decoding\u001b[0m\n",
      "TagLib: MP4: No audio tracks\n",
      "[\u001b[32;1m00007f4814cc82a0\u001b[0m] avcodec decoder: \u001b[0;1mUsing NVIDIA VDPAU Driver Shared Library  470.141.03  Thu Jun 30 18:33:34 UTC 2022 for hardware decoding\u001b[0m\n",
      "TagLib: MP4: No audio tracks\n",
      "[\u001b[32;1m00007f4814c17090\u001b[0m] avcodec decoder: \u001b[0;1mUsing NVIDIA VDPAU Driver Shared Library  470.141.03  Thu Jun 30 18:33:34 UTC 2022 for hardware decoding\u001b[0m\n",
      "The X11 connection broke: I/O error (code 1)\n",
      "XIO:  fatal IO error 11 (Resource temporarily unavailable) on X server \":1\"\n",
      "      after 54 requests (54 known processed) with 0 events remaining.\n"
     ]
    }
   ],
   "source": [
    "!ffmpeg -y -framerate 10 -pattern_type glob -i \"{OUTPUT_DIR}/*.jpg\"  \\\n",
    "    -c:v libx264 -pix_fmt yuv420p -vf trim=0:30 \"{OUTPUT_DIR}/movie.mp4\"\n",
    "\n",
    "# Play the movie with vlc (if it's installed)\n",
    "!vlc \"{OUTPUT_DIR}/movie.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLC media player 3.0.16 Vetinari (revision 3.0.16-0-g5e70837d8d)\n",
      "[\u001b[32;1m00005603960e6a00\u001b[0m] main libvlc: \u001b[0;1mRunning vlc with the default interface. Use 'cvlc' to use vlc without interface.\u001b[0m\n",
      "TagLib: MP4: No audio tracks\n",
      "Qt: Session management error: Could not open network socket\n",
      "TagLib: MP4: No audio tracks\n",
      "libva info: VA-API version 1.1.0\n",
      "libva info: va_getDriverName() returns 0\n",
      "libva info: Trying to open /snap/vlc/2344/usr/lib/x86_64-linux-gnu/dri/nvidia_drv_video.so\n",
      "libva info: Found init function __vaDriverInit_1_0\n",
      "libva info: va_openDriver() returns 0\n",
      "[\u001b[32;1m00007feaa0001f30\u001b[0m] glconv_vaapi_x11 gl error: \u001b[31;1mvaCreateSurfaces: attribute not supported\u001b[0m\n",
      "[\u001b[32;1m00007feaa804c120\u001b[0m] main video output error: \u001b[31;1mvideo output creation failed\u001b[0m\n",
      "[\u001b[32;1m00007feac0c03fa0\u001b[0m] main decoder error: \u001b[31;1mfailed to create video output\u001b[0m\n",
      "[\u001b[32;1m00007feac0c03fa0\u001b[0m] avcodec decoder: \u001b[0;1mUsing NVIDIA VDPAU Driver Shared Library  470.141.03  Thu Jun 30 18:33:34 UTC 2022 for hardware decoding\u001b[0m\n",
      "TagLib: MP4: No audio tracks\n",
      "[\u001b[32;1m00007feac0c4bc10\u001b[0m] avcodec decoder: \u001b[0;1mUsing NVIDIA VDPAU Driver Shared Library  470.141.03  Thu Jun 30 18:33:34 UTC 2022 for hardware decoding\u001b[0m\n",
      "TagLib: MP4: No audio tracks\n",
      "[\u001b[32;1m00007feac0c48050\u001b[0m] avcodec decoder: \u001b[0;1mUsing NVIDIA VDPAU Driver Shared Library  470.141.03  Thu Jun 30 18:33:34 UTC 2022 for hardware decoding\u001b[0m\n",
      "TagLib: MP4: No audio tracks\n",
      "[\u001b[32;1m00007feac0c173e0\u001b[0m] avcodec decoder: \u001b[0;1mUsing NVIDIA VDPAU Driver Shared Library  470.141.03  Thu Jun 30 18:33:34 UTC 2022 for hardware decoding\u001b[0m\n",
      "QObject::~QObject: Timers cannot be stopped from another thread\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('stable-diffusion-im2im')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cea3af3ea9aced30a5c72c792fbd026032c096800e9cc9471dd580bb4e3cffca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
