{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import diffusers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from diffusers import LMSDiscreteScheduler, StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "from skimage.exposure import match_histograms\n",
    "from torch import autocast\n",
    "from torchvision import transforms as tfms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from img2img import StableDiffusionImg2ImgPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STABLE_DIFFUSION_MODEL_PATH = Path.home() / \"Desktop/stable-diffusion-v1-4\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n",
      "{'trained_betas'} was not found in config. Values will be initialized to default values.\n"
     ]
    }
   ],
   "source": [
    "# Load the txt_to_img pipeline\n",
    "txt_to_img = StableDiffusionPipeline.from_pretrained(\n",
    "    str(STABLE_DIFFUSION_MODEL_PATH), revision=\"fp16\", torch_dtype=torch.float16\n",
    ")\n",
    "# Turn off safety_checker to avoid false positives\n",
    "txt_to_img.safety_checker = lambda images, **kwargs: (images, False)\n",
    "txt_to_img.enable_attention_slicing()  # use less vram\n",
    "txt_to_img = txt_to_img.to(device)\n",
    "\n",
    "# Load the img2img pipeline, using the models\n",
    "# from the txt_to_img pipeline, to not waste vram.\n",
    "im2im = StableDiffusionImg2ImgPipeline(\n",
    "    vae=txt_to_img.vae,\n",
    "    text_encoder=txt_to_img.text_encoder,\n",
    "    tokenizer=txt_to_img.tokenizer,\n",
    "    unet=txt_to_img.unet,\n",
    "    scheduler=LMSDiscreteScheduler(\n",
    "        beta_start=0.00085,\n",
    "        beta_end=0.012,\n",
    "        beta_schedule=\"scaled_linear\",\n",
    "        num_train_timesteps=1000,\n",
    "    ),\n",
    ")\n",
    "im2im.enable_attention_slicing()\n",
    "im2im.progress_bar = lambda iterable: iterable  # turn off progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "\n",
    "\n",
    "def timestamp():\n",
    "    return time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "\n",
    "def maintain_colors(prev_img, color_match_sample, mode):\n",
    "    # source: https://colab.research.google.com/github/deforum/stable-diffusion/blob/main/Deforum_Stable_Diffusion.ipynb#scrollTo=2g-f7cQmf2Nt\n",
    "    if mode == \"Match Frame 0 RGB\":\n",
    "        return match_histograms(prev_img, color_match_sample, multichannel=True)\n",
    "    elif mode == \"Match Frame 0 HSV\":\n",
    "        prev_img_hsv = cv2.cvtColor(prev_img, cv2.COLOR_RGB2HSV)\n",
    "        color_match_hsv = cv2.cvtColor(color_match_sample, cv2.COLOR_RGB2HSV)\n",
    "        matched_hsv = match_histograms(prev_img_hsv, color_match_hsv, multichannel=True)\n",
    "        return cv2.cvtColor(matched_hsv, cv2.COLOR_HSV2RGB)\n",
    "    else:  # Match Frame 0 LAB\n",
    "        prev_img_lab = cv2.cvtColor(prev_img, cv2.COLOR_RGB2LAB)\n",
    "        color_match_lab = cv2.cvtColor(color_match_sample, cv2.COLOR_RGB2LAB)\n",
    "        matched_lab = match_histograms(prev_img_lab, color_match_lab, multichannel=True)\n",
    "        return cv2.cvtColor(matched_lab, cv2.COLOR_LAB2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "MOVIE_DIR = Path(f\"images/{timestamp()}\")\n",
    "PROMPT_A = \"A photo of a bowl of fruit\"\n",
    "PROMPT_B = \"A photo of an acrobat\"\n",
    "GUIDANCE_SCALE = 7.5\n",
    "IMG2IMG_STRENGTH = 0.44\n",
    "NUM_IMG2IMG_STEPS = 1000\n",
    "SEED = 0\n",
    "\n",
    "MOVIE_DIR.mkdir(exist_ok=True, parents=True)\n",
    "generator = torch.Generator(\"cuda\").manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 29/51 [00:05<00:04,  5.35it/s]"
     ]
    }
   ],
   "source": [
    "# Generate the initial image\n",
    "with autocast(\"cuda\"), torch.no_grad():\n",
    "    init_image = txt_to_img([PROMPT_A], generator=generator)[\"sample\"][0]\n",
    "init_image.save(MOVIE_DIR / f\"{PROMPT_A}_{PROMPT_B}_{0:04d}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/999 [00:00<?, ?it/s]/tmp/ipykernel_30867/570955215.py:11: FutureWarning: `multichannel` is a deprecated argument name for `match_histograms`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  return match_histograms(prev_img, color_match_sample, multichannel=True)\n",
      " 12%|█▏        | 118/999 [08:18<1:02:03,  4.23s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mGenerator(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmanual_seed(i)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m), torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 11\u001b[0m     image \u001b[38;5;241m=\u001b[39m im2im(\n\u001b[1;32m     12\u001b[0m         prompt,\n\u001b[1;32m     13\u001b[0m         image,\n\u001b[1;32m     14\u001b[0m         strength\u001b[38;5;241m=\u001b[39mSTRENGTH,\n\u001b[1;32m     15\u001b[0m         guidance_scale\u001b[38;5;241m=\u001b[39mGUIDANCE_SCALE,\n\u001b[1;32m     16\u001b[0m         generator\u001b[38;5;241m=\u001b[39mgenerator,\n\u001b[1;32m     17\u001b[0m     )[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     18\u001b[0m image\u001b[38;5;241m.\u001b[39msave(movie_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minitial_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m04d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/stable-diffusion-im2im/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/stable-diffusion-im2im/img2img.py:263\u001b[0m, in \u001b[0;36mStableDiffusionImg2ImgPipeline.__call__\u001b[0;34m(self, prompt, init_image, strength, num_inference_steps, guidance_scale, eta, generator, output_type, return_dict)\u001b[0m\n\u001b[1;32m    260\u001b[0m     t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39munet\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    262\u001b[0m \u001b[39m# predict the noise residual\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m noise_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munet(latent_model_input, t, encoder_hidden_states\u001b[39m=\u001b[39;49mtext_embeddings)\u001b[39m.\u001b[39msample\n\u001b[1;32m    265\u001b[0m \u001b[39m# perform guidance\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39mif\u001b[39;00m do_classifier_free_guidance:\n",
      "File \u001b[0;32m~/miniconda3/envs/stable-diffusion-im2im/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/stable-diffusion-im2im/lib/python3.9/site-packages/diffusers/models/unet_2d_condition.py:251\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    248\u001b[0m down_block_res_samples \u001b[39m=\u001b[39m down_block_res_samples[: \u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(upsample_block\u001b[39m.\u001b[39mresnets)]\n\u001b[1;32m    250\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(upsample_block, \u001b[39m\"\u001b[39m\u001b[39mattentions\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m upsample_block\u001b[39m.\u001b[39mattentions \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     sample \u001b[39m=\u001b[39m upsample_block(\n\u001b[1;32m    252\u001b[0m         hidden_states\u001b[39m=\u001b[39;49msample,\n\u001b[1;32m    253\u001b[0m         temb\u001b[39m=\u001b[39;49memb,\n\u001b[1;32m    254\u001b[0m         res_hidden_states_tuple\u001b[39m=\u001b[39;49mres_samples,\n\u001b[1;32m    255\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    257\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     sample \u001b[39m=\u001b[39m upsample_block(hidden_states\u001b[39m=\u001b[39msample, temb\u001b[39m=\u001b[39memb, res_hidden_states_tuple\u001b[39m=\u001b[39mres_samples)\n",
      "File \u001b[0;32m~/miniconda3/envs/stable-diffusion-im2im/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/stable-diffusion-im2im/lib/python3.9/site-packages/diffusers/models/unet_blocks.py:1084\u001b[0m, in \u001b[0;36mCrossAttnUpBlock2D.forward\u001b[0;34m(self, hidden_states, res_hidden_states_tuple, temb, encoder_hidden_states)\u001b[0m\n\u001b[1;32m   1081\u001b[0m     res_hidden_states_tuple \u001b[39m=\u001b[39m res_hidden_states_tuple[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m   1082\u001b[0m     hidden_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([hidden_states, res_hidden_states], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m-> 1084\u001b[0m     hidden_states \u001b[39m=\u001b[39m resnet(hidden_states, temb)\n\u001b[1;32m   1085\u001b[0m     hidden_states \u001b[39m=\u001b[39m attn(hidden_states, context\u001b[39m=\u001b[39mencoder_hidden_states)\n\u001b[1;32m   1087\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsamplers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/stable-diffusion-im2im/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/stable-diffusion-im2im/lib/python3.9/site-packages/diffusers/models/resnet.py:349\u001b[0m, in \u001b[0;36mResnetBlock2D.forward\u001b[0;34m(self, x, temb)\u001b[0m\n\u001b[1;32m    346\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(hidden_states)\n\u001b[1;32m    348\u001b[0m \u001b[39mif\u001b[39;00m temb \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m     temb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtime_emb_proj(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnonlinearity(temb))[:, :, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m    350\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m temb\n\u001b[1;32m    352\u001b[0m \u001b[39m# make sure hidden states is in float32\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[39m# when running in half-precision\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/stable-diffusion-im2im/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/stable-diffusion-im2im/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop from scratch\n",
    "\n",
    "image = init_image\n",
    "# Generate the rest of the images\n",
    "for i in tqdm(range(1, NUM_IMG2IMG_STEPS)):\n",
    "    # Try to prevent colours from going red\n",
    "    image = maintain_colors(np.array(image), np.array(init_image), \"Match Frame 0 RGB\")\n",
    "    image = Image.fromarray(image)\n",
    "    generator = torch.Generator(\"cuda\").manual_seed(i)\n",
    "    with autocast(\"cuda\"), torch.no_grad():\n",
    "        image = im2im(\n",
    "            PROMPT_B,\n",
    "            image,\n",
    "            strength=IMG2IMG_STRENGTH,\n",
    "            guidance_scale=GUIDANCE_SCALE,\n",
    "            generator=generator,\n",
    "        )[\"sample\"][0]\n",
    "    image.save(MOVIE_DIR / f\"{PROMPT_A}_{PROMPT_B}_{i:04d}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('stable-diffusion-im2im')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cea3af3ea9aced30a5c72c792fbd026032c096800e9cc9471dd580bb4e3cffca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
