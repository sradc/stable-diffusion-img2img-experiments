{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import diffusers\n",
    "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch import autocast\n",
    "from torchvision import transforms as tfms\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "STABLE_DIFFUSION_MODEL_PATH = Path.home() / \"Desktop/stable-diffusion-v1-4\"\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n",
      "{'trained_betas'} was not found in config. Values will be initialized to default values.\n"
     ]
    }
   ],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    str(STABLE_DIFFUSION_MODEL_PATH), revision=\"fp16\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe.safety_checker = lambda images, **kwargs: (images, False)  # turn off to avoid false positives\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = pipe.tokenizer\n",
    "text_encoder = pipe.text_encoder\n",
    "unet = pipe.unet\n",
    "vae = pipe.vae\n",
    "scheduler = LMSDiscreteScheduler(\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    num_train_timesteps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img import StableDiffusionImg2ImgPipeline\n",
    "from transformers import CLIPFeatureExtractor\n",
    "\n",
    "from diffusers.pipelines.stable_diffusion.safety_checker import StableDiffusionSafetyChecker\n",
    "\n",
    "# safety_cheker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(self.device)\n",
    "# image, has_nsfw_concept = self.safety_checker(images=image, clip_input=safety_cheker_input.pixel_values)\n",
    "\n",
    "class FeatureExtractorDummy(CLIPFeatureExtractor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        return None\n",
    "\n",
    "safety_checker_dummy = lambda images, **kwargs: (images, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_modules(self, **kwargs):\n",
    "    for k, v in kwargs.items():\n",
    "        setattr(self, k, v)\n",
    "\n",
    "StableDiffusionImg2ImgPipeline.register_modules = register_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "im2im = StableDiffusionImg2ImgPipeline(\n",
    "    vae = vae,\n",
    "    text_encoder=text_encoder,\n",
    "    tokenizer=tokenizer,\n",
    "    unet=unet,\n",
    "    scheduler=scheduler,\n",
    "    safety_checker=safety_checker_dummy,\n",
    "    feature_extractor=FeatureExtractorDummy\n",
    ")\n",
    "im2im.progress_bar = lambda iterable: iterable  # turn off progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "\n",
    "timestamp = lambda: time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "from skimage.exposure import match_histograms\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def maintain_colors(prev_img, color_match_sample, mode):\n",
    "    # source: https://colab.research.google.com/github/deforum/stable-diffusion/blob/main/Deforum_Stable_Diffusion.ipynb#scrollTo=2g-f7cQmf2Nt\n",
    "    if mode == 'Match Frame 0 RGB':\n",
    "        return match_histograms(prev_img, color_match_sample, multichannel=True)\n",
    "    elif mode == 'Match Frame 0 HSV':\n",
    "        prev_img_hsv = cv2.cvtColor(prev_img, cv2.COLOR_RGB2HSV)\n",
    "        color_match_hsv = cv2.cvtColor(color_match_sample, cv2.COLOR_RGB2HSV)\n",
    "        matched_hsv = match_histograms(prev_img_hsv, color_match_hsv, multichannel=True)\n",
    "        return cv2.cvtColor(matched_hsv, cv2.COLOR_HSV2RGB)\n",
    "    else: # Match Frame 0 LAB\n",
    "        prev_img_lab = cv2.cvtColor(prev_img, cv2.COLOR_RGB2LAB)\n",
    "        color_match_lab = cv2.cvtColor(color_match_sample, cv2.COLOR_RGB2LAB)\n",
    "        matched_lab = match_histograms(prev_img_lab, color_match_lab, multichannel=True)\n",
    "        return cv2.cvtColor(matched_lab, cv2.COLOR_LAB2RGB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "initial_prompt = \"A photo of a bowl of fruit\"\n",
    "prompt = \"A photo of an acrobat\"\n",
    "N = 1000\n",
    "\n",
    "movie_dir = Path(f\"images/{timestamp()}\")\n",
    "movie_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "generator = torch.Generator(\"cuda\").manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/51 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 7.79 GiB total capacity; 2.07 GiB already allocated; 275.06 MiB free; 2.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate the initial image\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m), torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 3\u001b[0m     init_image \u001b[38;5;241m=\u001b[39m pipe([initial_prompt], generator\u001b[38;5;241m=\u001b[39mgenerator)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/stable-diffusion-im2im/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/stable-diffusion-im2im/lib/python3.9/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:249\u001b[0m, in \u001b[0;36mStableDiffusionPipeline.__call__\u001b[0;34m(self, prompt, height, width, num_inference_steps, guidance_scale, eta, generator, latents, output_type, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     latent_model_input \u001b[39m=\u001b[39m latent_model_input \u001b[39m/\u001b[39m ((sigma\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m)\n\u001b[1;32m    248\u001b[0m \u001b[39m# predict the noise residual\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m noise_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munet(latent_model_input, t, encoder_hidden_states\u001b[39m=\u001b[39;49mtext_embeddings)\u001b[39m.\u001b[39msample\n\u001b[1;32m    251\u001b[0m \u001b[39m# perform guidance\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[39mif\u001b[39;00m do_classifier_free_guidance:\n",
      "File \u001b[0;32m~/miniconda3/envs/stable-diffusion-im2im/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/stable-diffusion-im2im/lib/python3.9/site-packages/diffusers/models/unet_2d_condition.py:234\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mfor\u001b[39;00m downsample_block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_blocks:\n\u001b[1;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(downsample_block, \u001b[39m\"\u001b[39m\u001b[39mattentions\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m downsample_block\u001b[39m.\u001b[39mattentions \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m         sample, res_samples \u001b[39m=\u001b[39m downsample_block(\n\u001b[1;32m    235\u001b[0m             hidden_states\u001b[39m=\u001b[39;49msample, temb\u001b[39m=\u001b[39;49memb, encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states\n\u001b[1;32m    236\u001b[0m         )\n\u001b[1;32m    237\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m         sample, res_samples \u001b[39m=\u001b[39m downsample_block(hidden_states\u001b[39m=\u001b[39msample, temb\u001b[39m=\u001b[39memb)\n",
      "File \u001b[0;32m~/miniconda3/envs/stable-diffusion-im2im/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/stable-diffusion-im2im/lib/python3.9/site-packages/diffusers/models/unet_blocks.py:537\u001b[0m, in \u001b[0;36mCrossAttnDownBlock2D.forward\u001b[0;34m(self, hidden_states, temb, encoder_hidden_states)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[39mfor\u001b[39;00m resnet, attn \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresnets, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattentions):\n\u001b[1;32m    536\u001b[0m     hidden_states \u001b[39m=\u001b[39m resnet(hidden_states, temb)\n\u001b[0;32m--> 537\u001b[0m     hidden_states \u001b[39m=\u001b[39m attn(hidden_states, context\u001b[39m=\u001b[39;49mencoder_hidden_states)\n\u001b[1;32m    538\u001b[0m     output_states \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (hidden_states,)\n\u001b[1;32m    540\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsamplers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/stable-diffusion-im2im/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/stable-diffusion-im2im/lib/python3.9/site-packages/diffusers/models/attention.py:148\u001b[0m, in \u001b[0;36mSpatialTransformer.forward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    146\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mreshape(b, h \u001b[39m*\u001b[39m w, c)\n\u001b[1;32m    147\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_blocks:\n\u001b[0;32m--> 148\u001b[0m     x \u001b[39m=\u001b[39m block(x, context\u001b[39m=\u001b[39;49mcontext)\n\u001b[1;32m    149\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mreshape(b, h, w, c)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    150\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj_out(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/stable-diffusion-im2im/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/stable-diffusion-im2im/lib/python3.9/site-packages/diffusers/models/attention.py:197\u001b[0m, in \u001b[0;36mBasicTransformerBlock.forward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, context\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    196\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mcontiguous() \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m x\n\u001b[0;32m--> 197\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1(x)) \u001b[39m+\u001b[39m x\n\u001b[1;32m    198\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x), context\u001b[39m=\u001b[39mcontext) \u001b[39m+\u001b[39m x\n\u001b[1;32m    199\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mff(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm3(x)) \u001b[39m+\u001b[39m x\n",
      "File \u001b[0;32m~/miniconda3/envs/stable-diffusion-im2im/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/stable-diffusion-im2im/lib/python3.9/site-packages/diffusers/models/attention.py:265\u001b[0m, in \u001b[0;36mCrossAttention.forward\u001b[0;34m(self, x, context, mask)\u001b[0m\n\u001b[1;32m    260\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreshape_heads_to_batch_dim(v)\n\u001b[1;32m    262\u001b[0m \u001b[39m# TODO(PVP) - mask is currently never used. Remember to re-implement when used\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \n\u001b[1;32m    264\u001b[0m \u001b[39m# attention, what we cannot get enough of\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attention(q, k, v, sequence_length, dim)\n\u001b[1;32m    267\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_out(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/stable-diffusion-im2im/lib/python3.9/site-packages/diffusers/models/attention.py:279\u001b[0m, in \u001b[0;36mCrossAttention._attention\u001b[0;34m(self, query, key, value, sequence_length, dim)\u001b[0m\n\u001b[1;32m    276\u001b[0m start_idx \u001b[39m=\u001b[39m i \u001b[39m*\u001b[39m slice_size\n\u001b[1;32m    277\u001b[0m end_idx \u001b[39m=\u001b[39m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m slice_size\n\u001b[1;32m    278\u001b[0m attn_slice \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 279\u001b[0m     torch\u001b[39m.\u001b[39;49meinsum(\u001b[39m\"\u001b[39;49m\u001b[39mb i d, b j d -> b i j\u001b[39;49m\u001b[39m\"\u001b[39;49m, query[start_idx:end_idx], key[start_idx:end_idx]) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale\n\u001b[1;32m    280\u001b[0m )\n\u001b[1;32m    281\u001b[0m attn_slice \u001b[39m=\u001b[39m attn_slice\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    282\u001b[0m attn_slice \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39mb i j, b j d -> b i d\u001b[39m\u001b[39m\"\u001b[39m, attn_slice, value[start_idx:end_idx])\n",
      "File \u001b[0;32m~/miniconda3/envs/stable-diffusion-im2im/lib/python3.9/site-packages/torch/functional.py:360\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[39m# recurse incase operands contains value that has torch function\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[39m# in the original implementation this line is omitted\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39m_operands)\n\u001b[0;32m--> 360\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49meinsum(equation, operands)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 7.79 GiB total capacity; 2.07 GiB already allocated; 275.06 MiB free; 2.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Generate the initial image\n",
    "with autocast(\"cuda\"), torch.no_grad():\n",
    "    init_image = pipe([initial_prompt], generator=generator)[\"sample\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop from scratch\n",
    "\n",
    "image = init_image\n",
    "# Generate the rest of the images\n",
    "for i in tqdm(range(1, N)):\n",
    "    image.save(movie_dir / f\"{initial_prompt}_{prompt}_{i:04d}.jpg\")\n",
    "    # Stop colours from going red\n",
    "    image = maintain_colors(np.array(image), np.array(init_image), 'Match Frame 0 RGB')\n",
    "    image = Image.fromarray(image)\n",
    "    generator = torch.Generator(\"cuda\").manual_seed(i)\n",
    "    with autocast(\"cuda\"), torch.no_grad():\n",
    "        image = im2im(prompt, image, strength=0.5, guidance_scale=7.5, generator=generator)[\"sample\"][0]\n",
    "\n",
    "image.save(movie_dir / f\"{initial_prompt}_{prompt}_{i + 1:04d}.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('stable-diffusion-im2im')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cea3af3ea9aced30a5c72c792fbd026032c096800e9cc9471dd580bb4e3cffca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
